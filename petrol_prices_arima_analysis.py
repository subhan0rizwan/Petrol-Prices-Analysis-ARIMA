# -*- coding: utf-8 -*-
"""Petrol Prices ARIMA Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112BMLSm6RT5CeR_h5NZTuUOswNbhBx4D

# **Introduction**

Fuel prices significantly impact the global economy and daily life. Accurate petrol price predictions are essential for government agencies, businesses, and consumers.

# **Project Objective**

This project aims to predict petrol prices using the ARIMA (AutoRegressive Integrated Moving Average) model, leveraging historical data to forecast future prices and aid decision-making.

# **Why ARIMA?**

ARIMA is effective for time series analysis, capturing temporal dependencies and handling trends and seasonalities, making it ideal for forecasting petrol prices.

# **Structure of the Project**

**Data Collection and Preprocessing:** Gather and clean historical petrol price data.
**Exploratory Data Analysis (EDA):** Visualize and analyze data patterns.

**ARIMA Model Building:** Develop and fine-tune the ARIMA model.

**Model Evaluation:** Assess model performance and accuracy.

**Forecasting:** Make future predictions and interpret results.

**Conclusion and Future Work:** Summarize findings and discuss improvements.

# **Significance**

Accurate petrol price predictions benefit businesses, policymakers, and consumers by aiding budgeting, regulatory decisions, and personal financial planning. This project demonstrates the practical application of the ARIMA model for meaningful price forecasting.

# Importing Libraries & Dataset
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA

from google.colab import drive
drive.mount('/content/drive', force_remount = True)

path = "/content/drive/MyDrive/TRAIN_~1.CSV"
df = pd.read_csv(path).set_index("Date")
df.dropna(inplace = True)
df.head(10)

"""# Exploratory Data Analysis (EDA):"""

resample_rule= "1M" #Resampling the data by Month
df = df[df["Petrol (USD)"] < 160] # Clearing all outliers above "160"
df.index = pd.to_datetime(df.index, format='%m/%d/%Y') # Changing the date format
df = df.resample(resample_rule).mean().fillna(method='ffill')
df.head()

df.plot()

import seaborn as sns
df["rolling"] = df["Petrol (USD)"].rolling(12).mean()
df["std"] = df["Petrol (USD)"].std()
sns.lineplot(data = df, x = df.index, y = "rolling", color = "green")
sns.lineplot(data = df, x = df.index, y = "std", color = "orange")
sns.lineplot(data = df, x = df.index, y = "Petrol (USD)")

"""By visualizing the historical petrol prices alongside their rolling mean and standard deviation, we can better understand the data's trends, smooth out noise, and assess the level of price volatility over time. This information is valuable for making informed decisions and developing predictive models.

# Data Preprocessing

After, calculating rolling mean and standard deviation, we'll calculate Augmented Dickey-Fuller (ADF) test to find out if the data is stationary or not. This statistical test determines whether the data possesses a unit root, indicating non-stationarity. Stationarity is crucial for accurate modeling and forecasting.
"""

from statsmodels.tsa.stattools import adfuller
adfTest = adfuller(df["Petrol (USD)"], autolag = "AIC")
print(f"ADF Statistic: {adfTest[0]}")
print(f"p-value: {adfTest[1]}")

adfTest

for key, values in adfTest[4].items():
  print(f"Critical Value {key}: {values}")

"""As we see the p-value is bigger than the criticali values. In that case, it indicates that the dataset is not stationary, following the ADF test, the data will undergo various transformations. These include shifting the data by 1 time step, logarithmic transformation, square root transformation, cube root transformation, and a combination of logarithmic, shifting, and square root transformations. These transformations aim to stabilize variance and achieve stationarity, enhancing the performance of time series models such as ARIMA."""

def test_stationarity(dataFrame, var):
    dataFrame['rollMean']  = dataFrame[var].rolling(window=12).mean()
    dataFrame['rollStd']  = dataFrame[var].rolling(window=12).std()

    from statsmodels.tsa.stattools import adfuller
    adfTest = adfuller(dataFrame[var],autolag='AIC')
    stats = pd.Series(adfTest[0:4],index=['Test Statistic','p-value','#lags used','number of observations used'])
    print(stats)

    for key, values in adfTest[4].items():
        print('criticality',key,":",values)
    fig, ax = plt.subplots(figsize=(15, 6))
    sns.lineplot(data=dataFrame,x=dataFrame.index,y=var, ax = ax)
    sns.lineplot(data=dataFrame,x=dataFrame.index,y='rollMean', ax = ax)
    sns.lineplot(data=dataFrame,x=dataFrame.index,y='rollStd', ax = ax)

"""# Shifting the data by 1"""

air_df = df[["Petrol (USD)"]]
air_df.head()

air_df["shift"] = air_df["Petrol (USD)"].shift(4)
air_df["shiftDiff"] = air_df["Petrol (USD)"] - air_df["shift"]
air_df.head()
air_df = air_df.dropna()
air_df.head()

test_stationarity(air_df, "shiftDiff")

"""The provided statistics indicate the results of the Augmented Dickey-Fuller (ADF) test performed on the shifted petrol price data. The test statistic of -3.656470 falls below the critical values at the 1%, 5%, and 10% significance levels, suggesting strong evidence against the null hypothesis of non-stationarity. With 13 lags used and 169 observations in the analysis, the p-value of 0.004769 further supports the rejection of the null hypothesis, indicating that the shifted data is stationary. However, despite achieving stationarity through shifting, further data transformations, such as applying the square root transformation, may still be necessary to stabilize variance and improve the model's performance. The square root transformation can help address any remaining non-linear patterns or heteroscedasticity in the data, enhancing its suitability for time series modeling and forecasting.

# Transforming the data by log
"""

log_df = df[["Petrol (USD)"]]
log_df["log"] = np.log(log_df["Petrol (USD)"])
log_df.head()

test_stationarity(log_df, "log")

"""The Augmented Dickey-Fuller (ADF) test results for the log-transformed petrol price data show a test statistic of -1.992339 and a p-value of 0.289942. These values indicate insufficient evidence to reject the null hypothesis of non-stationarity, as the test statistic does not fall below the critical values at common significance levels. Despite the lack of stationarity achieved through the log transformation, further data transformations may be needed to stabilize variance and improve model performance. Moving towards a square root transformation could address any remaining non-linear patterns or heteroscedasticity in the data, potentially enhancing its suitability for time series modeling and forecasting.

# Transforming the data by Square Root
"""

sqrt_df = df[["Petrol (USD)"]]
sqrt_df["sqrt"] = np.sqrt(df[["Petrol (USD)"]])
sqrt_df.head()

test_stationarity(sqrt_df, "sqrt")

"""The Augmented Dickey-Fuller (ADF) test results for the square root-transformed petrol price data show a test statistic of -2.006318 and a p-value of 0.283810. These statistics suggest insufficient evidence to reject the null hypothesis of non-stationarity, as the test statistic does not fall below the critical values at common significance levels. Since the square root transformation alone did not achieve stationarity, considering alternative transformations such as the cube root may be warranted. Transforming the data by the cube root could further stabilize variance and address any remaining non-linear patterns or heteroscedasticity, potentially enhancing the data's suitability for time series modeling and forecasting.

# Transforming the data by Cube Root
"""

cbrt_df = df[["Petrol (USD)"]]
cbrt_df["cbrt"] = np.cbrt(df[["Petrol (USD)"]])
cbrt_df.head()

test_stationarity(cbrt_df, "cbrt")

"""The Augmented Dickey-Fuller (ADF) test results for the cube root-transformed petrol price data reveal a test statistic of -1.932910 and a p-value of 0.316741. These statistics indicate insufficient evidence to reject the null hypothesis of non-stationarity, as the test statistic does not fall below the critical values at common significance levels. Given that the cube root transformation alone did not achieve stationarity, exploring a combined transformation approach involving shift 1, log, and square root transformations may be beneficial. Integrating these transformations aims to comprehensively address any remaining non-linear patterns or heteroscedasticity in the data, potentially enhancing its suitability for time series modeling and forecasting by improving stability and reducing volatility.

# Transforming the data by Combination of Shift1, Log & Square Root
"""

log_df2 = log_df[['Petrol (USD)','log']]
log_df2['log_sqrt'] = np.sqrt(log_df['log'])
log_df2['logShiftDiff'] = log_df2['log_sqrt'] - log_df2['log_sqrt'].shift()
log_df2.dropna(inplace = True)
log_df2.head()

test_stationarity(log_df2, "logShiftDiff")

"""The combination of shift 1, log, and square root transformations applied to the petrol price data yields compelling results, with a test statistic of -4.161535 and a p-value of 0.000765. These values indicate strong evidence of stationarity, as the test statistic falls significantly below the critical values at common significance levels. This comprehensive transformation approach effectively mitigates non-linear patterns and heteroscedasticity in the data, leading to improved stability and suitability for time series modeling and forecasting. This analysis represents a significant advancement in the preprocessing stage, ensuring the data's readiness for accurate and reliable predictions.

# Further EDA
"""

log_shift = log_df[["Petrol (USD)"]].shift(1)
log_shift.head()

df.head()

petrol_df = df[["Petrol (USD)"]].copy(True)
petrol_df["Diff1"] = petrol_df["Petrol (USD)"].diff()
petrol_df["Diff2"] = petrol_df["Petrol (USD)"].diff(12)
petrol_df.dropna(inplace = True)
petrol_df.head()

fig, ax = plt.subplots(figsize =(15, 6))
plot_acf(petrol_df["Diff1"], ax =ax)
plt.show()

fig, ax = plt.subplots(figsize =(15, 6))
plot_pacf(petrol_df["Diff1"], ax =ax)
plt.show()

# p = 1, q = 1, d = 1

"""The selection of p = 1, q = 1, and d = 1 for the ARIMA model was based on the examination of the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots. Significant spikes at lag 1 in both plots, along with the gradual decay of correlations, indicated the need for differencing the series once (d = 1) to achieve stationarity. Furthermore, the presence of a significant spike at lag 1 in the PACF plot suggested the inclusion of one autoregressive term (p = 1) in the model. Additionally, the significant autocorrelation at lag 1 in the ACF plot hinted at the necessity of one moving average term (q = 1) to capture the remaining autocorrelation not explained by the autoregressive term. These parameter values were chosen to construct an ARIMA model that effectively captures the autocorrelation structure of the time series data.

#Splitting the Data
"""

cutoff = int(len(petrol_df["Petrol (USD)"]) * 0.65)
train, test = petrol_df["Petrol (USD)"][:cutoff], petrol_df["Petrol (USD)"][cutoff:]

train_mean = train.mean()
train_pred_baseline = [train_mean] * len(train)
mae = mean_absolute_error(train, train_pred_baseline)
print(f"MAE: {mae}")
print(f"Baseline Mean: {train_mean}")

p_params = range(0, 24, 4)
q_params = range(0, 3, 1)

"""#Building the Model"""

model = ARIMA(train, order = (1, 1, 1)).fit()
prediction = model.predict(start = test.index[0], end = test.index[-1])
petrol_df["ARIMA"] = prediction
sns.lineplot(data = petrol_df, x = petrol_df.index, y = "Petrol (USD)")
sns.lineplot(data = prediction, color = "orange");

from sklearn.metrics import mean_squared_error
mse_model1 = mean_squared_error(test, prediction)
print(f"MSE Model 1 : {mse_model1}")

"""With an MSE of 146.36 for Model 1, its performance is deemed unsatisfactory due to the significant deviation between predicted and actual values. Consequently, a different ARIMA model is being pursued to improve accuracy. This shift entails adjusting parameters or exploring advanced methods like SARIMA to better capture data patterns and enhance prediction quality, aiming for a lower MSE.

#Training the 2nd Model
"""

import time
import warnings
warnings.filterwarnings("ignore")

# Create dictionary to store MAEs
mae_grid = dict()
# Outer loop: Iterate through possible values for `p`
for p in p_params:
    # Create key-value pair in dict. Key is `p`, value is empty list.
    mae_grid[p] = list()
    # Inner loop: Iterate through possible values for `q`
    for q in q_params:
        # Combination of hyperparameters for model
        order = (p, 0, q)
        # Note start time
        start_time = time.time()
        # Train model
        model = ARIMA(train, order = order).fit()
        # Calculate model training time
        elapsed_time = round(time.time() - start_time, 2)
        print(f"Trained ARIMA {order} in {elapsed_time} seconds.")
        # Generate in-sample (training) predictions
        pred = model.predict()
        # Calculate training MAE
        mae = mean_absolute_error(train, pred)
        # Append MAE to list in dictionary
        mae_grid[p].append(mae)

print()
print(mae_grid)

mae_df = pd.DataFrame(mae_grid)
mae_df.head()

"""#Model Evaluation

Now we'll use a heatmap to visually assess the performance of different ARIMA model configurations, showcasing their AIC or BIC values. Darker colors or lower values in the heatmap signify better model fit. By examining the heatmap, we'll swiftly pinpoint the most suitable combination of autoregressive (p), differencing (d), and moving average (q) parameters that effectively capture the data's underlying patterns.

Additionally, we'll employ the plot_diagnostics function to generate diagnostic plots, which offer insights into the ARIMA model's performance. These plots include visualizations for standardized residuals over time, histogram plus estimated density to evaluate normality, normal Q-Q plots for comparing residual distribution to a normal distribution, and correlograms (ACF plots) to check for autocorrelation. Through these diagnostic plots, we'll assess if the model assumptions, such as randomness, normality, and independence of residuals, are met, enabling us to identify any potential deficiencies in the model.
"""

import seaborn as sns
sns.heatmap(mae_df)
plt.xlabel("p values")
plt.ylabel("q values");

fig, ax = plt.subplots(figsize=(15, 12))
model.plot_diagnostics(fig =fig )
plt.show()

"""#Communicating the results"""

import plotly.express as px
pred_wfv = pd.Series(dtype=float)

history = train.copy()

# Walk-forward validation
for i in range(len(test)):
    # Fit the model on the current history
    model = ARIMA(history, order=(1, 1, 1)).fit()

    # Forecast the next value
    next_pred = model.forecast(steps=20)

    # Store the forecasted value
    pred_wfv = pd.concat([pred_wfv, pd.Series(next_pred, index=[test.index[i]])])

    # Append the actual test value to the history
    history = pd.concat([history, test.iloc[i:i+1]])

# Reset the index of the predictions series if necessary
pred_wfv = pred_wfv.reset_index(drop=True)

# Calculate Mean Absolute Error (MAE)
test_mae = mean_absolute_error(test, pred_wfv)
mae = mean_absolute_error(train, train_pred_baseline)
print("Test MAE (walk forward validation):", round(test_mae, 2))
print("Baseline MAE:", round(mae, 2))

# Create DataFrame for predictions
df_predictions = pd.DataFrame({
    "test": test.reset_index(drop=True),
    "pred_wfv": pred_wfv.reset_index(drop=True)
})

# Plot the predictions
fig = px.line(df_predictions, title="Test vs Predicted values (Walk-forward Validation)")
fig.show()

"""#Final Evaluation"""

from sklearn.metrics import mean_squared_error
mse_model2 = mean_squared_error(test, pred_wfv)
print(f"MSE Model 2 : {mse_model2}")

"""With an MSE of 3.8723905889394667 for Model 2, the research concludes with promising results indicating the desired outcome. This substantially lower MSE suggests that Model 2 outperforms previous iterations, demonstrating improved accuracy in predicting petrol prices. By achieving a lower MSE, the model effectively minimizes the squared differences between predicted and actual values, signifying better overall performance and enhanced predictive capabilities. Consequently, these findings validate the efficacy of the chosen ARIMA model configuration and parameter selection methodology. As a result, the research successfully achieves its objective of developing a reliable model for petrol price prediction, providing valuable insights for stakeholders in the energy sector and facilitating informed decision-making processes.







"""

